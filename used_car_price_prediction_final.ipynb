{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/qsmr/GH_collab/blob/main/used_car_price_prediction_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sN2jIkBnJVfc"
      },
      "source": [
        "# **USED CAR PRICE PREDICTION**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AeYgqRVQMX9Y"
      },
      "source": [
        "**`Procedere con l'esecuzione di ogni singola cella aspettando che termini la computazione.`**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIwLw_JMFL4f"
      },
      "source": [
        "%matplotlib inline\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qf1OakMpxpSV"
      },
      "source": [
        "### Download file for plotting complexity curves"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-G7ctRK5avI",
        "outputId": "abfe6011-1ccc-42e8-8f64-245d54e6cad3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!curl https://raw.githubusercontent.com/francescopisu/Used-car-price-prediction/master/curves.py --output curves.py"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  3760  100  3760    0     0   7163      0 --:--:-- --:--:-- --:--:--  7161\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdjJ9_p8x1d2"
      },
      "source": [
        "### Downlod dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCqDQfZOyCWq",
        "outputId": "db935d6d-6c28-4690-d056-0528f065257a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# May take up to 5 minutes depending on connection\n",
        "!curl https://raw.githubusercontent.com/francescopisu/Used-car-price-prediction/master/data/cars.csv --output cars.csv"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 86.2M  100 86.2M    0     0  12.6M      0  0:00:06  0:00:06 --:--:-- 23.8M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLTa_BGjFQwK",
        "outputId": "2f387cfa-fef4-4a40-c25f-6781b74716d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install spark-sklearn\n",
        "!pip install pyspark"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting spark-sklearn\n",
            "  Downloading spark-sklearn-0.3.0.tar.gz (28 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting scikit-learn<0.20,>=0.18.1 (from spark-sklearn)\n",
            "  Downloading scikit-learn-0.19.2.tar.gz (9.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: spark-sklearn, scikit-learn\n",
            "  Building wheel for spark-sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for spark-sklearn: filename=spark_sklearn-0.3.0-py3-none-any.whl size=30577 sha256=db2369f412037eeff4fb12fe2189b8eb4d0091d01cba83681669ed896f74297d\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/c3/49/1d965f070cae453ee0192dc86655721447970d1572377f5e3a\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for scikit-learn (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for scikit-learn\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py clean for scikit-learn\n",
            "Successfully built spark-sklearn\n",
            "Failed to build scikit-learn\n",
            "\u001b[31mERROR: Could not build wheels for scikit-learn, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting pyspark\n",
            "  Downloading pyspark-3.4.1.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.1-py2.py3-none-any.whl size=311285398 sha256=790c6f6c1955fde9c85f41f0f7214422b585ff5737a34c2dc35156a01c34a4fa\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/77/a3/ff2f74cc9ab41f8f594dabf0579c2a7c6de920d584206e0834\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94Ic9RORidUu",
        "outputId": "6bea04c8-d706-4123-e6e4-b4679d97017f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "pip install rfpimp"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rfpimp\n",
            "  Downloading rfpimp-1.3.7.tar.gz (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rfpimp) (1.22.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from rfpimp) (1.5.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from rfpimp) (1.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from rfpimp) (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->rfpimp) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->rfpimp) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->rfpimp) (4.41.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->rfpimp) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->rfpimp) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->rfpimp) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->rfpimp) (3.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->rfpimp) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->rfpimp) (2022.7.1)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->rfpimp) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->rfpimp) (1.3.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->rfpimp) (3.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->rfpimp) (1.16.0)\n",
            "Building wheels for collected packages: rfpimp\n",
            "  Building wheel for rfpimp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rfpimp: filename=rfpimp-1.3.7-py3-none-any.whl size=10649 sha256=5aec06078f6b93d33e390a8888eba0402ed38104d2e5ac6b2534aa8c023ab348\n",
            "  Stored in directory: /root/.cache/pip/wheels/6a/12/08/d5bc35127c8d69d39c1f3736a95419ab4763cc0c80ed65bf41\n",
            "Successfully built rfpimp\n",
            "Installing collected packages: rfpimp\n",
            "Successfully installed rfpimp-1.3.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPYPU1EXDxPz"
      },
      "source": [
        "**IMPORTING LIBRARIES**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pgN1TBSanwD",
        "outputId": "ad14d863-95fd-4b8c-ff3f-7776b4a721c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        }
      },
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import datetime\n",
        "import matplotlib\n",
        "import numpy as np\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import curves\n",
        "from sklearn import preprocessing\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, Normalizer\n",
        "from spark_sklearn import GridSearchCV\n",
        "from spark_sklearn.util import createLocalSparkSession\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import mean_squared_error"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-949783856b40>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNormalizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mspark_sklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mspark_sklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcreateLocalSparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'spark_sklearn'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXFaluefD5Oo"
      },
      "source": [
        "**READ THE CSV INPUT FILE**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnRTEzfLIQIK"
      },
      "source": [
        "# Read data\n",
        "cars = pd.read_csv('cars.csv', encoding='latin1', error_bad_lines=False, warn_bad_lines=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BeOPNIqTIgvM"
      },
      "source": [
        "cars.sample(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaQinJiw-hOZ"
      },
      "source": [
        "cars.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlOA6L-i-XPg"
      },
      "source": [
        "## ANALYSING PRICE ATTRIBUTE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2zLC7p7-cd3"
      },
      "source": [
        "cars['Price'].describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSH504GV_QfO"
      },
      "source": [
        "sns.distplot(cars['Price'])\n",
        "#skewness and kurtosis\n",
        "print(\"Skewness: %f\" % cars['Price'].skew())\n",
        "print(\"Kurtosis: %f\" % cars['Price'].kurt())\n",
        "\n",
        "\"\"\"\n",
        "We can observe that the distribution of prices shows a high positive skewness\n",
        "to the left (skew > 1). A kurtosis value of 60 is extremely high, meaning that\n",
        "there is a profusion of outliers in the dataset. We need to do something.\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZyYU6_xA_Kt"
      },
      "source": [
        "### RELATIONSHIP WITH NUMERICAL FEATURES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcCzZRoRBDiz"
      },
      "source": [
        "# Scatter plot Mileage / Price\n",
        "attrib = 'Mileage'\n",
        "data = pd.concat([cars['Price'], cars[attrib]], axis=1)\n",
        "data.plot.scatter(x=attrib, y='Price', ylim=(0,450000));\n",
        "\n",
        "\"\"\"\n",
        "We see that the lesser is the mileage, the higher tends to be the price.\n",
        "Price and Mileage seem to be in an exponential relationship with negative\n",
        "exponent. It's a typical form of exponential decay.\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlAAbgqkEKOb"
      },
      "source": [
        "# Scatter plot Year / Price\n",
        "attrib = 'Year'\n",
        "data = pd.concat([cars['Price'], cars[attrib]], axis=1)\n",
        "data.plot.scatter(x=attrib, y='Price', ylim=(0,450000));\n",
        "\n",
        "\"\"\"\n",
        "Prices tend to be higher as cars are more recent, and viceversa.\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBihFTU-FrKs"
      },
      "source": [
        "### RELATIONSHIP WITH CATEGORICAL FEATURES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNtZZXkyGH5a"
      },
      "source": [
        "# Box plot Make (Car manufacturers) / Price\n",
        "\n",
        "attrib = 'Make'\n",
        "data = pd.concat([cars['Price'], cars[attrib]], axis=1)\n",
        "f, ax = plt.subplots(figsize=(20, 12))\n",
        "fig = sns.boxplot(x=attrib, y=\"Price\", data=data)\n",
        "fig.axis(ymin=0, ymax=450000);\n",
        "\n",
        "\"\"\"\n",
        "We can observe there is a correlation between Price and Exotic/Luxury car manufacturers.\n",
        "On a sidenote, there is a prevalence of low to medium budget cars in the dataset\n",
        "\n",
        "Several car manufacturers as Porsche, Audi, Chevreolet ecc.. have a strong\n",
        "presence of outliers, altought outliers may not be the right term because\n",
        "those makers produce car models that cost more than others.\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArdRAIyVJhGs"
      },
      "source": [
        "# Box plot Year / Price\n",
        "\n",
        "attrib = 'Year'\n",
        "data = pd.concat([cars['Price'], cars[attrib]], axis=1)\n",
        "f, ax = plt.subplots(figsize=(18, 12))\n",
        "fig = sns.boxplot(x=attrib, y=\"Price\", data=data)\n",
        "fig.axis(ymin=0, ymax=450000);\n",
        "\n",
        "\"\"\"\n",
        "We can observe a slight increase of Prices the most recent years. Not a strong\n",
        "tendency though.\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpEMxGM-a-M_"
      },
      "source": [
        "# Box plot State / Price\n",
        "\n",
        "attrib = 'State'\n",
        "data = pd.concat([cars['Price'], cars[attrib]], axis=1)\n",
        "f, ax = plt.subplots(figsize=(18, 12))\n",
        "fig = sns.boxplot(x=attrib, y=\"Price\", data=data)\n",
        "fig.axis(ymin=0, ymax=450000);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIH-ljJNBeSn"
      },
      "source": [
        "### FEATURE IMPORTANCE RELATED TO TARGET\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1p-G6T6YBjJj"
      },
      "source": [
        "# Find most important features relative to target Price\n",
        "print(\"Find most important features relative to target\")\n",
        "corr = cars.corr()\n",
        "corr.sort_values([\"Price\"], ascending = False, inplace = True)\n",
        "print(corr.Price)\n",
        "\n",
        "\"\"\"\n",
        "Between Year and Price there is a positive correlation, meaning that the higher\n",
        "is the Year (more recent), the higher is the Price (more recent cars cost more).\n",
        "Between Price and Mileage there is a negative correlation, meaning that higher\n",
        "is the mileage, lower is the Price (cars with high mileage cost less).\n",
        "\n",
        "We obviously will get rid of Id attribute.\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d23Q7vENLbFb"
      },
      "source": [
        "### CORRELATION MATRIX"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2OwPM36LaGu"
      },
      "source": [
        "# General correlation matrix\n",
        "corrmat = cars.corr()\n",
        "f, ax = plt.subplots(figsize=(7, 7))\n",
        "sns.heatmap(corrmat, vmax=.8, square=True);\n",
        "\n",
        "\"\"\"\n",
        "There is a mild positive correlation between Year and Price and a mild\n",
        "negative correlation between Year and Mileage (as we have already said).\n",
        "Moreover the is a strong negative correlation between Mileage and Year,\n",
        "meaning that the older is the car, the higher will be its mileage.\n",
        "Obviously there are exceptions but that is the general trend.\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IO-lgUyUP2W3"
      },
      "source": [
        "# Price correlation matrix\n",
        "k = 4 #number of variables for heatmap\n",
        "cols = corrmat.nlargest(k, 'Price')['Price'].index\n",
        "cm = np.corrcoef(cars[cols].values.T)\n",
        "sns.set(font_scale=1.25)\n",
        "hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\n",
        "plt.show()\n",
        "\n",
        "\"\"\"\n",
        "This plot shows what we said earies but in form of numbers.\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j04bG-PRQ7Nl"
      },
      "source": [
        "### SCATTERPLOTS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6itEJQr_Q6mo"
      },
      "source": [
        "# Scatter plot for numerical features\n",
        "num_features = [\"Price\", \"Mileage\", \"Year\"]\n",
        "sns.pairplot(cars[num_features], size = 2.5)\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzLPddsrD_oQ"
      },
      "source": [
        "## **PREPROCESSING**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3eqqW2nKGnJ"
      },
      "source": [
        "# Checking for missing values -> no missing values\n",
        "cars.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPUDlHMrKGxC"
      },
      "source": [
        "# Checking for plausible values of numerical features -> we can see that the maximum value of mileage is 77milion km\n",
        "# we need to fix these problems\n",
        "cars.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxsqetnyVxc6"
      },
      "source": [
        "# Record distribution over the year\n",
        "sns.distplot((cars[\"Year\"]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qf-N3ej_NCEJ"
      },
      "source": [
        "# Checking values of categorical features\n",
        "cat_val = [\"City\", \"State\", \"Vin\", \"Make\",\"Model\"]\n",
        "for col in cat_val:\n",
        "    print ([col],\" : \",cars[col].unique())\n",
        "\n",
        "# Features like Id, State and Vin can be discarded because wouldn't bring anything good to our purpose"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZjr-AcaTM5F"
      },
      "source": [
        "### OUTLIERS MANAGEMENT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gW8ropifUp9T"
      },
      "source": [
        "**BIVARIATE ANALYSIS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4WYwzCQUlHU"
      },
      "source": [
        "# Bivariate analysis Price / Year\n",
        "attrib = 'Year'\n",
        "data = pd.concat([cars['Price'], cars[attrib]], axis=1)\n",
        "data.plot.scatter(x=attrib, y='Price', ylim=(0,420000));"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfraprD9XkQs"
      },
      "source": [
        "**REMOVING OUTLIERS BY MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSquBEmqdrPa"
      },
      "source": [
        "# Box Plot showing car manufacturer / Price range\n",
        "\n",
        "attrib = 'Make'\n",
        "data = pd.concat([cars['Price'], cars[attrib]], axis=1)\n",
        "f, ax = plt.subplots(figsize=(20, 12))\n",
        "fig = sns.boxplot(x=attrib, y=\"Price\", data=data)\n",
        "fig.axis(ymin=0, ymax=450000);\n",
        "\n",
        "\"\"\"\n",
        "We can see a certain number of outliers. Now we'll try to get rid of those.\n",
        "\"\"\"\n",
        "\n",
        "gender, tissue_type, tissue_value\n",
        "F, 'cerebrum_l_wm', 10\n",
        "M, 'cerebrum_r_gm', 20\n",
        "F, 'cerebrum_r_wm', 10\n",
        "M, 'cerebrum_r_gm', 20\n",
        "F, 'cerebrum_l_gm', 10\n",
        "M, 'cerebrum_l_gm', 20\n",
        "F, 'cerebrum_r_wm', 10\n",
        "M, 'cerebrum_r_gm', 20\n",
        "\n",
        "boxplot(x=\"tissue_type\", y=\"tissue_value\", hue=\"gender\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckYGP37Memvw"
      },
      "source": [
        "from numpy import percentile\n",
        "from functools import partial\n",
        "\n",
        "category='Model'\n",
        "\n",
        "models = cars[category].unique()\n",
        "\n",
        "\"\"\"\n",
        "Here we are taking the values between the 25th and 80th percentile of the\n",
        "gaussian curve.\n",
        "\"\"\"\n",
        "p25 = cars.groupby(by=category)[category,'Price'].agg(lambda x: percentile(x['Price'], 20))\n",
        "p80 = cars.groupby(by=category)[category,'Price'].agg(lambda x: percentile(x['Price'], 80))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZBvwMDGe4we"
      },
      "source": [
        "# Now this strategy is applied to the 3k models; it will take up to 10 minutes\n",
        "# depending on the hardware\n",
        "models = cars[category].unique()\n",
        "\n",
        "cars_cleaned = pd.DataFrame()\n",
        "\n",
        "for i,m in enumerate(models):\n",
        "    cars_cleaned=cars_cleaned.append(cars[( (cars[category] == m ) & ( (cars[\"Price\"] > p25.Model[m]) & (cars[\"Price\"] <= p80.Model[m])) ) ],ignore_index=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3RDyKjXiKP0"
      },
      "source": [
        "cars_cleaned.to_csv('cars_outliers_removed.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ympO-2k30ekY"
      },
      "source": [
        "### ATTENTION\n",
        "Alternatively, the cleaned_dataset can be downloaded executing the cell above:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xE8NtIwg0nYR"
      },
      "source": [
        "!curl https://raw.githubusercontent.com/francescopisu/Used-car-price-prediction/master/data/cars_outliers_removed.csv --output cars_outliers_removed.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YLJ6INJjAXD"
      },
      "source": [
        "# Reading the newly cleaned dataset\n",
        "cleaned_cars = pd.read_csv(\"cars_outliers_removed.csv\", encoding='latin1', error_bad_lines=False,warn_bad_lines=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oS6nyhg5TFZE"
      },
      "source": [
        "# Box Plot showing car manufacturer / Price range after cleaning\n",
        "\n",
        "attrib = 'Make'\n",
        "data = pd.concat([cleaned_cars['Price'], cleaned_cars[attrib]], axis=1)\n",
        "f, ax = plt.subplots(figsize=(20, 12))\n",
        "fig = sns.boxplot(x=attrib, y=\"Price\", data=data)\n",
        "fig.axis(ymin=0, ymax=450000);\n",
        "\n",
        "\"\"\"\n",
        "The outliers have been eliminated.\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdra3dND7n9E"
      },
      "source": [
        "print(\"Too new: %d\" % cleaned_cars.loc[cleaned_cars.Year >= 2017].count()['Id'])\n",
        "print(\"Too few km: \" , cleaned_cars.loc[cleaned_cars.Mileage < 5000].count()['Id'])\n",
        "print(\"Too many km: \" , cleaned_cars.loc[cleaned_cars.Mileage > 250000].count()['Id'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3zJ8VhJJ5W6"
      },
      "source": [
        "# Drop some unuseful columns with respect to our analysis\n",
        "cleaned_cars = cleaned_cars.drop([\"Id\", \"State\", \"Vin\", \"City\"], axis=1)\n",
        "\n",
        "# Replace the NaN values for categoric attributes\n",
        "cleaned_cars['Make'].fillna(value='blank', inplace=True)\n",
        "cleaned_cars['Model'].fillna(value='blank', inplace=True)\n",
        "\n",
        "\n",
        "# Drop duplicates\n",
        "cleaned_cars = cleaned_cars.drop_duplicates([\"Year\", \"Mileage\", \"Price\", \"Make\", \"Model\"])\n",
        "\n",
        "# Remove outliers\n",
        "cleaned_cars = cleaned_cars[\n",
        "        (cleaned_cars.Year <= 2017)\n",
        "      & (cleaned_cars.Year >= 2008)\n",
        "      & (cleaned_cars.Mileage >= 5000)\n",
        "      & (cleaned_cars.Mileage <= 250000)]\n",
        "\n",
        "# Remove extra column\n",
        "cleaned_cars = cleaned_cars.drop([\"Unnamed: 0\"], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVtSFuxxUw_Y"
      },
      "source": [
        "# Scatter plot for numerical features\n",
        "num_features = [\"Price\", \"Mileage\", \"Year\"]\n",
        "sns.pairplot(cleaned_cars[num_features], size = 2.5)\n",
        "plt.show();\n",
        "\n",
        "# In the Mileage/Price plot we observe that the lesser is the mileage, the higher\n",
        "# is the price. In the Year/Price plot instead, we observe that the prices\n",
        "# are somewhat equally distributed between the various years."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltSe37vDcYxj"
      },
      "source": [
        "cleaned_cars.sample(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lgdiyg5Gdsgy"
      },
      "source": [
        "### TOWARDS NORMAL DISTRIBUTION OF PRICES\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qq_J8fr6dxKo"
      },
      "source": [
        "# Distribution of prices\n",
        "from scipy.stats import norm\n",
        "from scipy import stats\n",
        "\n",
        "# Using Histogram and normal probability plot\n",
        "sns.distplot(cleaned_cars['Price'], fit=norm);\n",
        "fig = plt.figure()\n",
        "res = stats.probplot(cleaned_cars['Price'], plot=plt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbPsS5UCfKbz"
      },
      "source": [
        "#applying log transformation\n",
        "cleaned_cars['Price'] = np.log(cleaned_cars['Price'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WyagLqjlfPGt"
      },
      "source": [
        "#transformed histogram and normal probability plot\n",
        "sns.distplot(cleaned_cars['Price'], fit=norm);\n",
        "fig = plt.figure()\n",
        "res = stats.probplot(cleaned_cars['Price'], plot=plt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZpisBfoseNw"
      },
      "source": [
        "### LABEL ENCODING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvUFGtI_Da56"
      },
      "source": [
        "# LABEL ENCODING\n",
        "features = ['Make', 'Model']\n",
        "les = {}\n",
        "\n",
        "for f in features:\n",
        "  les[f] = preprocessing.LabelEncoder()\n",
        "  les[f] = les[f].fit(cleaned_cars[f])\n",
        "  cleaned_cars[f] = les[f].transform(cleaned_cars[f])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YddeRJL4zMXj"
      },
      "source": [
        "### TRAIN / TEST SPLIT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8REvWnjGcgBA"
      },
      "source": [
        "# Splitting the dataset into train and test sets\n",
        "train_set, test_set = train_test_split(cleaned_cars, test_size = 0.33, random_state = 42)\n",
        "\n",
        "\"\"\"\n",
        "The Test Set in this case is our Hold-out set that we'll be using later\n",
        "for final validation\n",
        "\"\"\"\n",
        "\n",
        "# Separating target labels from the rest\n",
        "cars_train = train_set.drop(\"Price\", axis=1) #train without target\n",
        "cars_price_train = train_set[\"Price\"].copy() #target\n",
        "\n",
        "cars_test  = test_set.drop(\"Price\", axis=1)\n",
        "cars_price_test = test_set[\"Price\"].copy()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5izDgbIdTNY"
      },
      "source": [
        "## **TRAINING AND COMPARING MODELS**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZ7C5E6bkILA"
      },
      "source": [
        "**BEST SCORE FUNCTION**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjiPl2APkGm1"
      },
      "source": [
        "# This function returns the best score achieved by the model over all the cv splits\n",
        "def best_score(forest, cv):\n",
        "  best_score = 0\n",
        "  for i in range(0, cv):\n",
        "    items = list(map(lambda x: abs(x), forest.cv_results_['split'+str(i)+'_test_score']))\n",
        "    arr = np.append(best_score, items)\n",
        "    best_score = max(arr)\n",
        "\n",
        "  return best_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AwMbLxIkO6K"
      },
      "source": [
        "**BEST PARAMS FUNCTION**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0CsOEWBYkO6L"
      },
      "source": [
        "# This functions returns the best combination of parameters, which allows to\n",
        "# get the best score\n",
        "def best_params(forest):\n",
        "  return forest.cv_results_['params'][forest.cv_results_['rank_test_score'][0]-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqhm-JiOuOTN"
      },
      "source": [
        "** PERFORMANCE METRIC FUNCTION **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1v3ZqmoBMsT"
      },
      "source": [
        "from sklearn.metrics import r2_score\n",
        "\n",
        "def performance_metric(y_true, y_predict):\n",
        "    \"\"\" Calculates and returns the performance score between\n",
        "        true (y_true) and predicted (y_predict) values based on the metric chosen. \"\"\"\n",
        "\n",
        "    score = r2_score(y_true, y_predict)\n",
        "\n",
        "    # Return the score\n",
        "    return score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Z-1tIt1t-QR"
      },
      "source": [
        "### **LINEAR REGRESSION **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TDgn4e8cx_T"
      },
      "source": [
        "from sklearn.preprocessing import Imputer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.pipeline import FeatureUnion\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "import os\n",
        "\n",
        "# Create a class to select numerical or categorical columns\n",
        "# since Scikit-Learn doesn't handle DataFrames yet\n",
        "\n",
        "class DFSelector(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, feature_names):\n",
        "        self.feature_names = feature_names\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "    def transform(self, X):\n",
        "        return X[self.feature_names].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzvviQOKc-bZ"
      },
      "source": [
        "# Setting categorical and numerical attributes\n",
        "cat_features = [\"Make\", \"Model\"]\n",
        "num_features = list(cars_train.drop(cat_features, axis=1))\n",
        "\n",
        "# Building the Pipelines for categorical and numerical dataframes\n",
        "numerical_pipeline = Pipeline([\n",
        "    (\"selector\", DFSelector(num_features)),\n",
        "    (\"std_scaler\", StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_pipeline = Pipeline([\n",
        "    (\"selector\", DFSelector(cat_features)),\n",
        "    (\"encoder\", OneHotEncoder(sparse=True))\n",
        "])\n",
        "\n",
        "# full_pipeline\n",
        "full_pipeline = FeatureUnion(transformer_list =[\n",
        "    (\"num_pipeline\", numerical_pipeline),\n",
        "    (\"cat_pipeline\", categorical_pipeline)\n",
        "])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vtgxb0hVdHHR"
      },
      "source": [
        "# Apply the full pipeline\n",
        "ohe_cars_train = full_pipeline.fit_transform(cars_train) # train set"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ch9sxrXdMcy"
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "sc = createLocalSparkSession().sparkContext\n",
        "\n",
        "model = LinearRegression()\n",
        "parameters = {'fit_intercept':[False], 'normalize':[True,False], 'copy_X':[True, False]}\n",
        "\n",
        "# Spark parallelized GridSearchCV for hyperparameter tuning\n",
        "gs = GridSearchCV(sc, estimator=model, param_grid=parameters, cv=3, n_jobs=-1, verbose=1, return_train_score=True)\n",
        "lin_reg = gs.fit(ohe_cars_train, cars_price_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hW0jL8ICjgg0"
      },
      "source": [
        "# Best CV parameters\n",
        "bp = best_params(lin_reg)\n",
        "best_params(lin_reg)\n",
        "\n",
        "lin_reg_model = LinearRegression(\n",
        "                              fit_intercept=bp[\"fit_intercept\"],\n",
        "                              normalize=bp[\"normalize\"],\n",
        "                              copy_X=bp[\"copy_X\"])\n",
        "%time lin_reg_model.fit(ohe_cars_train, cars_price_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4_7rCKPsyNw"
      },
      "source": [
        "# LinearRegression scores for price prediction\n",
        "ohe_cars_test = full_pipeline.transform(cars_test)\n",
        "\n",
        "print(\"Best Linear Regression parameters:\")\n",
        "print(bp)\n",
        "print(\"\\nLinear Regressor score without CV on train set: %.3f\" % lin_reg_model.score(ohe_cars_train, cars_price_train)) #score on train set\n",
        "print(\"Linear Regression score without CV on test set: %.3f\" % lin_reg_model.score(ohe_cars_test, cars_price_test)) # score on test set\n",
        "print(\"Linear Regression Best score with CV=3: %.3f\" % best_score(lin_reg, 3)) # -> best score on test set is high"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4R_9IuzgdbiB"
      },
      "source": [
        "# Prediction on whole training set\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "price_predictions_train = lin_reg_model.predict(ohe_cars_train) #using the whole training set for making prediction with the final model given by the best CV parameters\n",
        "\n",
        "# Reversing np.log operation\n",
        "price_predictions_train_normal = np.exp(price_predictions_train)\n",
        "cars_price_train_normal = np.exp(cars_price_train)\n",
        "\n",
        "# MSE between target values (i.e known) and predicted values\n",
        "lin_mse = mean_squared_error(cars_price_train_normal, price_predictions_train_normal)\n",
        "lin_rmse = np.sqrt(lin_mse)\n",
        "lin_rmse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIrfcKVElIRc"
      },
      "source": [
        "print(price_predictions_train_normal[580:590])\n",
        "print('\\n')\n",
        "print(list(cars_price_train_normal[580:590]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkZnk4OKdd7z"
      },
      "source": [
        "# Prediction on test set\n",
        "price_predictions_test = lin_reg_model.predict(ohe_cars_test)\n",
        "\n",
        "# Reversing np.log operation\n",
        "price_predictions_test_normal = np.exp(price_predictions_test)\n",
        "cars_price_test_normal = np.exp(cars_price_test)\n",
        "\n",
        "# RMSE on test set\n",
        "final_mse = mean_squared_error(cars_price_test_normal, price_predictions_test_normal)\n",
        "final_rmse = np.sqrt(final_mse)\n",
        "\n",
        "final_rmse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4W3--eKhgNcU"
      },
      "source": [
        "print(price_predictions_test_normal[7650:7660]) #predictions on test set\n",
        "print('\\n')\n",
        "print(list(cars_price_test_normal[7650:7660])) #known values in test set"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42uCqjJqPAVb"
      },
      "source": [
        "# R2 score\n",
        "from sklearn.metrics import r2_score\n",
        "r2_score(cars_price_test_normal, price_predictions_test_normal, multioutput='variance_weighted')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crlgEF2jm417"
      },
      "source": [
        "# Plot predictions\n",
        "plt.scatter(price_predictions_train_normal, np.exp(cars_price_train), c = \"blue\", marker = \"s\", label = \"Training data\")\n",
        "plt.scatter(price_predictions_test_normal, np.exp(cars_price_test), c = \"lightgreen\", marker = \"s\", label = \"Validation data\")\n",
        "plt.title(\"Linear regression\")\n",
        "plt.xlabel(\"Predicted values\")\n",
        "plt.ylabel(\"Real values\")\n",
        "plt.legend(loc = \"upper left\")\n",
        "plt.plot([0, 350000], [0, 350000], c = \"red\")\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lh3bHHDrnJaZ"
      },
      "source": [
        "# Saving model for type prediction\n",
        "pickle.dump(lin_reg_model, open(\"lin_reg_model_final.pkl\", \"wb\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4mqxWfqnJad"
      },
      "source": [
        "# Loading the model for type prediction\n",
        "lin_reg_model = pickle.load(open(\"lin_reg_model_final.pkl\", 'rb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82-ztb_HzRBF"
      },
      "source": [
        "## **DECISION TREE REGRESSOR**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMinGxlLNrYN"
      },
      "source": [
        "### COMPLEXITY CURVES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58wTls3VlVGz"
      },
      "source": [
        "# Produce complexity curve for varying training set sizes and maximum depths\n",
        "curves.ModelComplexity_DT(cars_train, cars_price_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVYcQeg9_XkT"
      },
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import make_scorer\n",
        "\n",
        "def DT_SparkizedGridSearchCV(X, y):\n",
        "    \"\"\" Performs grid search over the 'max_depth' parameter for a\n",
        "        decision tree regressor trained on the input data [X, y]. \"\"\"\n",
        "\n",
        "    # Create cross-validation sets from the training data\n",
        "    cv_sets = ShuffleSplit(n_splits = 10, test_size = 0.20, random_state = 42)\n",
        "\n",
        "    # Create a decision tree regressor object\n",
        "    regressor = DecisionTreeRegressor()\n",
        "\n",
        "    # Create a dictionary for the parameter 'max_depth' with a range from 1 to 10\n",
        "    params = {'max_depth':[1, 5, 10, 15, 16, 17]}\n",
        "\n",
        "    # Transform 'performance_metric' into a scoring function using 'make_scorer'\n",
        "    scoring_fnc = make_scorer(performance_metric)\n",
        "\n",
        "    # Create the grid search cv object --> GridSearchCV()\n",
        "    sc = createLocalSparkSession().sparkContext\n",
        "    grid = GridSearchCV(sc, estimator=regressor, param_grid=params, scoring=scoring_fnc, cv=cv_sets)\n",
        "\n",
        "    # Fit the grid search object to the data to compute the optimal model\n",
        "    tree_reg = grid.fit(X, y)\n",
        "\n",
        "    # Return grid search output after fittig the data\n",
        "    return tree_reg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLgVJnlAAco8"
      },
      "source": [
        "from sklearn.model_selection import ShuffleSplit\n",
        "\n",
        "# Fit the training data to the model using spark parallelized grid search CV\n",
        "tree_reg = DT_SparkizedGridSearchCV(cars_train, cars_price_train)\n",
        "\n",
        "# Takign best parameters\n",
        "bp = best_params(tree_reg)\n",
        "\n",
        "# Produce the optimal value for 'max_depth'\n",
        "print(\"Parameter 'max_depth' is {} for the optimal model.\".format(bp['max_depth']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulCp9_-nlIKv"
      },
      "source": [
        "\"\"\"\n",
        "Due to the limitation of the spark-sklearn library's implementation of\n",
        "GridSearchCV, best_estimator_ parameter it's not available, so we need to\n",
        "fit a DecisionTreeRegressor on the best parameters given to us by gridSearchCV\n",
        "\"\"\"\n",
        "tree_reg_model = DecisionTreeRegressor(\n",
        "                              max_depth=bp['max_depth'])\n",
        "%time tree_reg_model.fit(cars_train, cars_price_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtz23jn9lIH9"
      },
      "source": [
        "# DecisionTreeRegressor score for price prediction\n",
        "\n",
        "\n",
        "print(\"Best Decision Tree Regressor parameters:\")\n",
        "print(bp)\n",
        "print(\"\\nDecision Tree Regressor score without CV on train set: %.3f\" % tree_reg_model.score(cars_train, cars_price_train)) #score on train set\n",
        "print(\"Decision Tree Regressor score without CV on test set: %.3f\" % tree_reg_model.score(cars_test, cars_price_test)) # score on test set\n",
        "print(\"Decision Tree Regressor Best score with CV=10: %.3f\" % best_score(tree_reg, 10)) # -> best score on test set is high"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTzbp7nklIFX"
      },
      "source": [
        "# Prediction on whole training set\n",
        "price_predictions_train = tree_reg_model.predict(cars_train) #using the whole training set for making prediction with the final model given by the best CV parameters\n",
        "\n",
        "# Reversing np.log operation\n",
        "price_predictions_train_normal = np.exp(price_predictions_train)\n",
        "cars_price_train_normal = np.exp(cars_price_train)\n",
        "\n",
        "# MSE between target values (i.e known) and predicted values\n",
        "lin_mse = mean_squared_error(cars_price_train_normal, price_predictions_train_normal)\n",
        "lin_rmse = np.sqrt(lin_mse)\n",
        "lin_rmse # is higher than RMSE of linear regression, in fact the best score is smaller (0.58 vs 0.89)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ie3JJ--K3H43"
      },
      "source": [
        "print(price_predictions_train_normal[1670:1680])\n",
        "print('\\n')\n",
        "print(list(cars_price_train_normal[1670:1680]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nlUGpGRhW5V"
      },
      "source": [
        "# Prediction on test set\n",
        "price_predictions_test = tree_reg_model.predict(cars_test)\n",
        "\n",
        "# reversing np.log operation\n",
        "price_predictions_test_normal = np.exp(price_predictions_test)\n",
        "cars_price_test_normal = np.exp(cars_price_test)\n",
        "\n",
        "# RMSE on test set\n",
        "final_mse = mean_squared_error(cars_price_test_normal, price_predictions_test_normal)\n",
        "final_rmse = np.sqrt(final_mse)\n",
        "\n",
        "final_rmse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1qEltq2hW5a"
      },
      "source": [
        "print(price_predictions_test_normal[1939:1949]) #predictions on test set\n",
        "print('\\n')\n",
        "print(list(cars_price_test_normal[1939:1949])) #known values in test set"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSy2VhUUNki3"
      },
      "source": [
        "# R2 score\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "r2_score(cars_price_test_normal, price_predictions_test_normal, multioutput='variance_weighted')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LV7OxboHq1Ou"
      },
      "source": [
        "# Saving model for type prediction\n",
        "pickle.dump(tree_reg_model, open(\"tree_reg_model_final.pkl\", \"wb\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mh-b4z9q1Oy"
      },
      "source": [
        "# Loading the model for type prediction\n",
        "tree_reg_model = pickle.load(open(\"tree_reg_model_final.pkl\", 'rb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28DNOQHc33BC"
      },
      "source": [
        "## **RANDOM FOREST REGRESSOR**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvr_pQyxz-35"
      },
      "source": [
        "### COMPLEXITY CURVES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cY_XNXO0LBh"
      },
      "source": [
        "# Produce complexity curve for varying training set sizes and maximum depths\n",
        "curves.ModelComplexity_RF(cars_train, cars_price_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgSRstziswd6"
      },
      "source": [
        "### Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpleScb00LBm"
      },
      "source": [
        "# Import 'make_scorer', 'DecisionTreeRegressor', and 'GridSearchCV'\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import make_scorer\n",
        "\n",
        "def RF_SparkizedGridSearchCV(X, y):\n",
        "    \"\"\" Performs grid search over the 'max_depth' parameter for a\n",
        "        decision tree regressor trained on the input data [X, y]. \"\"\"\n",
        "\n",
        "    # Create cross-validation sets from the training data\n",
        "    cv_sets = ShuffleSplit(n_splits = 10, test_size = 0.20, random_state = 42)\n",
        "\n",
        "    # Create a decision tree regressor object\n",
        "    regressor = RandomForestRegressor()\n",
        "\n",
        "    # Create a dictionary for the parameter 'max_depth' with a range from 1 to 10\n",
        "    params = {'max_depth':[16, 17, 18]}\n",
        "\n",
        "    # Transform 'performance_metric' into a scoring function using 'make_scorer'\n",
        "    scoring_fnc = make_scorer(performance_metric)\n",
        "\n",
        "    # Create the grid search cv object --> GridSearchCV()\n",
        "    sc = createLocalSparkSession().sparkContext\n",
        "    grid = GridSearchCV(sc, estimator=regressor, param_grid=params, scoring=scoring_fnc, cv=cv_sets)\n",
        "\n",
        "    # Fit the grid search object to the data to compute the optimal model\n",
        "    tree_reg = grid.fit(X, y)\n",
        "\n",
        "    # Return the best parameters after fitting the data\n",
        "    return tree_reg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_h0DIqn0LBo"
      },
      "source": [
        "from sklearn.model_selection import ShuffleSplit\n",
        "\n",
        "# Fit the training data to the model using spark parallelized grid search CV\n",
        "forest_reg = RF_SparkizedGridSearchCV(cars_train, cars_price_train)\n",
        "\n",
        "# Takign best parameters\n",
        "bp = best_params(forest_reg)\n",
        "\n",
        "# Produce the optimal value for 'max_depth'\n",
        "print(\"Parameter 'max_depth' is {} for the optimal model.\".format(bp['max_depth']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-kfF8VjlICt"
      },
      "source": [
        "# Fitting the forest\n",
        "\n",
        "forest_reg_model = RandomForestRegressor(\n",
        "                              max_depth=bp['max_depth']\n",
        "\n",
        ")\n",
        "\n",
        "%time forest_reg_model.fit(cars_train, cars_price_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKGLVyJx5SeD"
      },
      "source": [
        "# RandomForestRegressor score for price prediction\n",
        "\n",
        "print(bp)\n",
        "print(\"\\nRandom Forest Regressor score without CV on train set: %.3f\" % forest_reg_model.score(cars_train, cars_price_train)) #score on train set\n",
        "print(\"Random Forest Regressor score without CV on test set: %.3f\" % forest_reg_model.score(cars_test, cars_price_test)) #score on test set\n",
        "print(\"Random Forest Regressor Best score with CV=4: %.3f\" % best_score(forest_reg, 4)) # -> best score on test set is high"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGs7xINS5Sj_"
      },
      "source": [
        "# Prediction on whole training set\n",
        "price_predictions_train = forest_reg_model.predict(cars_train) #using the whole training set for making prediction with the final model given by the best CV parameters\n",
        "\n",
        "# Reversing np.log operation\n",
        "price_predictions_train_normal = np.exp(price_predictions_train)\n",
        "cars_price_train_normal = np.exp(cars_price_train)\n",
        "\n",
        "# MSE between target values (i.e known) and predicted values\n",
        "lin_mse = mean_squared_error(cars_price_train_normal, price_predictions_train_normal)\n",
        "lin_rmse = np.sqrt(lin_mse)\n",
        "lin_rmse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLaWB-GzD2Rd"
      },
      "source": [
        "print(price_predictions_train_normal[25670:25680])\n",
        "print('\\n')\n",
        "print(list(cars_price_train_normal[25670:25680]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjytVOPVjBKZ"
      },
      "source": [
        "# Prediction on test set\n",
        "price_predictions_test = forest_reg_model.predict(cars_test)\n",
        "\n",
        "# Reversing np.log operation\n",
        "price_predictions_test_normal = np.exp(price_predictions_test)\n",
        "cars_price_test_normal = np.exp(cars_price_test)\n",
        "\n",
        "final_mse = mean_squared_error(cars_price_test_normal, price_predictions_test_normal)\n",
        "final_rmse = np.sqrt(final_mse)\n",
        "\n",
        "final_rmse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fx1bGZNtjDac"
      },
      "source": [
        "print(price_predictions_test_normal[1870:1880]) #predictions on test set\n",
        "print('\\n')\n",
        "print(list(cars_price_test_normal[1870:1880])) #known values in test set"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsluMeS-lWUq"
      },
      "source": [
        "# r2 score between hold out prices and predicted prices\n",
        "r2_score(cars_price_test_normal, price_predictions_test_normal, multioutput='variance_weighted')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WR1zvT04qkO"
      },
      "source": [
        "# Saving model for type prediction\n",
        "pickle.dump(forest_reg_model, open(\"forest_reg_model_final.pkl\", \"wb\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Zi0HZlp4qkQ"
      },
      "source": [
        "# Loading the model for type prediction\n",
        "forest_reg_model = pickle.load(open(\"forest_reg_model_final.pkl\", 'rb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGwimNH5B6sG"
      },
      "source": [
        "## **CROSS VALIDATION**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTGotLf8Cooa"
      },
      "source": [
        "def display_scores(scores):\n",
        "    print(\"Scores:\", scores)\n",
        "    print(\"Mean:\", scores.mean())\n",
        "    print(\"Standard deviation:\", scores.std())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pE7S1tymiyX"
      },
      "source": [
        "### LINEAR REGRESSION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQjEyuqR0Xta"
      },
      "source": [
        "# Cross val score on training set, although we already used GridSearchCV\n",
        "\n",
        "train_scores = cross_val_score(lin_reg_model, ohe_cars_train, np.exp(cars_price_train),\n",
        "                         scoring=\"neg_mean_squared_error\", cv=10)\n",
        "tree_rmse_scores = np.sqrt(-train_scores)\n",
        "\n",
        "display_scores(tree_rmse_scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32b70U_Z0Xtd"
      },
      "source": [
        "train_scores = cross_val_score(lin_reg_model, ohe_cars_train, np.exp(cars_price_train),\n",
        "                          cv=10)\n",
        "\n",
        "display_scores(train_scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHYqKJQhmrYh"
      },
      "source": [
        "### DECISION TREE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1l2DA89mtm1"
      },
      "source": [
        "# Cross val score on training set\n",
        "\n",
        "train_scores = cross_val_score(tree_reg_model, cars_train, np.exp(cars_price_train),\n",
        "                         scoring=\"neg_mean_squared_error\", cv=10)\n",
        "tree_rmse_scores = np.sqrt(-train_scores)\n",
        "\n",
        "display_scores(tree_rmse_scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIsJAbwToaPG"
      },
      "source": [
        "train_scores = cross_val_score(tree_reg_model, cars_train, np.exp(cars_price_train),\n",
        "                          cv=10)\n",
        "\n",
        "display_scores(train_scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuYQUo8umuE0"
      },
      "source": [
        "###RANDOM FOREST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xorVHVx3mvgf"
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "\n",
        "# Cross val score on training set, although we already used grid search CV\n",
        "train_scores = cross_val_score(forest_reg_model, cars_train, np.exp(cars_price_train),\n",
        "                         scoring=\"neg_mean_squared_error\", cv=KFold(10, shuffle=True))\n",
        "forest_rmse_scores = np.sqrt(-train_scores)\n",
        "\n",
        "display_scores(forest_rmse_scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5hQY_Swtwt1"
      },
      "source": [
        "train_scores = cross_val_score(forest_reg_model, cars_train, np.exp(cars_price_train),\n",
        "                          cv=5)\n",
        "\n",
        "display_scores(train_scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26RTi437-vNm"
      },
      "source": [
        "## **PREDICTIONS ON FINAL MODEL AND CONCLUSIONS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lN0h4LGN-yIQ"
      },
      "source": [
        "final_model = forest_reg_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Exlfhol0qu7"
      },
      "source": [
        "**FEATURE IMPORTANCE**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLS3tICQ0puV"
      },
      "source": [
        "# Feature importance computed with cross validation\n",
        "from rfpimp import cv_importances, plot_importances\n",
        "%time I = cv_importances(final_model, cars_train, np.exp(cars_price_train), k=5)\n",
        "plot_importances(I)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbGTYtf20n41"
      },
      "source": [
        "# Prediction on the hold out test set\n",
        "final_predictions = final_model.predict(cars_test)\n",
        "\n",
        "final_mse = mean_squared_error(np.exp(cars_price_test), np.exp(final_predictions))\n",
        "\n",
        "final_rmse = np.sqrt(final_mse)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brzgbwJA-yO_"
      },
      "source": [
        "# RMSE and scores\n",
        "print(\"Test RMSE: %f \" % final_rmse)\n",
        "print(\"Score on held-out Test Set: %f \" % final_model.score(cars_test, cars_price_test))\n",
        "print(\"R2 Score: %f\" % r2_score(np.exp(cars_price_test), np.exp(final_predictions)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKBrSwCod7uQ"
      },
      "source": [
        "# Cross validation on the entire dataset, since we are good with our final model\n",
        "\n",
        "features = cleaned_cars.drop(['Price'], axis=1)\n",
        "prices = cleaned_cars['Price'].copy()\n",
        "\n",
        "final_rmses= cross_val_score(final_model, features, np.exp(prices),\n",
        "                          scoring=\"neg_mean_squared_error\", cv=KFold(10, shuffle=True))\n",
        "\n",
        "final_rmse_scores = np.sqrt(-final_rmses)\n",
        "display_scores(final_rmse_scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lh1QkfJ1tYcr"
      },
      "source": [
        "# Final cross validation scores in percentage, computed on the whole dataset\n",
        "final_scores = cross_val_score(tree_reg_model, features, np.exp(prices),\n",
        "                          cv=KFold(10, shuffle=True))\n",
        "\n",
        "display_scores(final_scores)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}